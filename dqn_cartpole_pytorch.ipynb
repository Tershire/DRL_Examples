{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a41d563-4b7a-4b0b-af34-65e07c07fb27",
   "metadata": {},
   "source": [
    "**dqn_cartpole_pytorch.ipynb**\n",
    "\n",
    "Wonhee Lee\n",
    "\n",
    "2024 MAY 05 (SUN)\n",
    "\n",
    "reference:\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa4725a-1892-4a6e-9651-d4ab3c05644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "356f5978-de93-4a48-9888-5342192c6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c830854f-a4fa-4f83-8ca8-16e746992446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce5c930-37ed-413a-84e2-eae4d4c5367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5947d1a-4b1e-43d4-97c9-b544769a4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21de9c0-806a-4a54-91bf-d596fe57334c",
   "metadata": {},
   "source": [
    "# environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1023837-ca84-4790-bbb9-6252862f6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383cf32d-0dbe-48e2-86fb-03f51fac9881",
   "metadata": {},
   "source": [
    "# replay memory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2c5a11d-3d74-4456-b538-5ada6383f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ea5827-c26a-40b2-aaa9-5d9f6a014eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"reward\", \"next_state\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57069a32-c6f2-4f95-9c04-c5135f51a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_Memory(object):  # object (?)\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f969f1f-b485-4736-b2df-7d0e0dd58135",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = Replay_Memory(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628c039-fc92-4ed3-ab3f-425ceff37839",
   "metadata": {},
   "source": [
    "# Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d11a42c-c25e-4f5d-8232-d3158e272bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89042b7e-a979-4550-bfaa-67eb51d2bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_observations, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_observations, 128),\n",
    "            nn.ReLU(0.1),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(0.1),\n",
    "            nn.Linear(128, num_actions))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8173a-3d89-44f0-a233-e1f6b99d9105",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa0f38-8b5e-4a32-9c65-03b6d8d0e0d2",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e626a36-9d94-4dbc-a089-5fdd6c9f6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory_batch_size = 128\n",
    "gamma = 0.99  # discount\n",
    "\n",
    "# epsilon-greedy\n",
    "epsilon_initial = 0.9\n",
    "epsilon_final = 0.05\n",
    "epsilon_decay_rate = 1000\n",
    "\n",
    "tau = 0.005  # target network update rate\n",
    "learning_rate = 1E-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153bd1af-b05c-4e76-adef-bc78a3bdc28f",
   "metadata": {},
   "source": [
    "## network setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55392c51-f797-4e24-9dc7-f2fc459d3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "\n",
    "observation, observation_info = env.reset()\n",
    "num_observations = len(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b8f01dc-dc08-46fe-ba34-aa16f17ea59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4\n"
     ]
    }
   ],
   "source": [
    "print(num_actions, num_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04ba020e-5165-4392-b0f9-87ba3472b81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = DQN(num_observations, num_actions).to(device)\n",
    "target_net = DQN(num_observations, num_actions).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())  # (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7ebf15f-1010-4666-84be-637a783af357",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(policy_net.parameters(), lr=learning_rate, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81ee2470-c899-4b20-8756-9b0c2ab61186",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "def select_action(state):\n",
    "    \"\"\"select action based on epsilon-greedy.\"\"\"\n",
    "    global steps_done\n",
    "    random_number = random.random()\n",
    "\n",
    "    epsilon_threshold = epsilon_final + (epsilon_initial - epsilon_final)*math.exp(-1.*steps_done/epsilon_decay_rate)\n",
    "\n",
    "    steps_done += 1\n",
    "\n",
    "    if random_number > epsilon_threshold:\n",
    "        with torch.no_grad():  # (!)            \n",
    "            return policy_net(state).max(1).indices.view(1, 1)  # (?)\n",
    "\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc54fd-59e7-4056-b53d-dd233cb565e4",
   "metadata": {},
   "source": [
    "## plot setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea0c0121-4217-4bf3-973b-66e69e2ce05b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807e06e1-4ef3-4a44-8d5c-e9effd1586f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import Dash, dcc, html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb0dbc-4706-4e7a-a739-caf84448bbb3",
   "metadata": {},
   "source": [
    "## method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43bceb7e-fb6f-455a-b64e-470056a89169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(replay_memory) < replay_memory_batch_size:\n",
    "        return  # (?)\n",
    "\n",
    "    transitions = replay_memory.sample(replay_memory_batch_size)\n",
    "    transition_batch = Transition(*zip(*transitions))  # (?)\n",
    "\n",
    "    # compute a mask of non-final states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, transition_batch.next_state)), device=device, dtype=torch.bool)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in transition_batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(transition_batch.state)\n",
    "    action_batch = torch.cat(transition_batch.action)\n",
    "    reward_batch = torch.cat(transition_batch.reward)\n",
    "\n",
    "    # Q(s_{t}, a)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # V(s_{t+1})\n",
    "    next_state_values = torch.zeros(replay_memory_batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    expected_state_action_values = reward_batch + gamma*next_state_values\n",
    "\n",
    "    # compute Huber loss\n",
    "    loss_function = nn.SmoothL1Loss()  # (?)\n",
    "    loss = loss_function(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # optimize\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)  # in-place gradient clipping\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eed4560e-34fd-49ea-8acf-1fb6bdb7a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d4e7ca9-9187-47c6-aed0-c144f652504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0317d575-9e0c-4578-b523-ab8043508c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: complete.\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    observation, observation_info = env.reset()\n",
    "    state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _= env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # store transition in replay memory\n",
    "        replay_memory.push(state, action, reward, next_state)\n",
    "\n",
    "        # step forward\n",
    "        state = next_state\n",
    "\n",
    "        # optimize\n",
    "        optimize_model()\n",
    "\n",
    "        # soft update of target network weights (?)\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = tau*policy_net_state_dict[key] + (1 - tau)*target_net_state_dict[key]\n",
    "            target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            # plot\n",
    "            break \n",
    "\n",
    "print(\"training: complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721dffe-8276-470a-b43e-d29afc7b7421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
